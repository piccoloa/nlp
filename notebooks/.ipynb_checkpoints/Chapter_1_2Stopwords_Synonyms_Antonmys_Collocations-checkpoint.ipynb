{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopwords** are common words that generally do not contribute to the meaning of a sentence,\n",
    "at least for the purposes of information retrieval and natural language processing  \n",
    "\n",
    "stopwords corpus is an instance of nltk.corpus.reader WordListCorpusReader. As such, it has a words() method that can take a single argument for the file ID, which in this case is 'english'  \n",
    "see the list of all English stopwords using stopwords.words('english') or by\n",
    "examining the word list file at nltk_data/corpora/stopwords/english. There are also\n",
    "stopword lists for many other languages. You can see the complete list of languages using the\n",
    "fileids method as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"stopwords_and_NLTK.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "words = [\"Can't\", 'is', 'a', 'contraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'contraction']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in words if word not in english_stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***WordNet*** is a lexical database for the English language. In other words, it's a dictionary designed specifically for natural language processing  \n",
    ">What you get is a list of Synset instances, which are groupings of synonymous words that express the same concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Synset for word cookbook***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the wordnet files are in the \\corpa\\wordnet folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookbook.n.01'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('cookbook')[0]\n",
    "syn.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition= a book of recipes and cooking directions \n",
      "examples= \n",
      " ['cooking can be a great art', 'people are needed who have experience in cookery', 'he left the preparation of meals to his wife']\n"
     ]
    }
   ],
   "source": [
    "print('definition=', syn.definition(), '\\nexamples= \\n', \n",
    "      wordnet.synsets('cooking')[0].examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synsets are organized in a structure similar to that of an inheritance tree   \n",
    "    More abstract terms are known as **hypernyms**\n",
    "    more specific terms are **hyponyms*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('reference_book.n.01')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('annual.n.02'),\n",
       " Synset('atlas.n.02'),\n",
       " Synset('cookbook.n.01'),\n",
       " Synset('directory.n.01'),\n",
       " Synset('encyclopedia.n.01'),\n",
       " Synset('handbook.n.01'),\n",
       " Synset('instruction_book.n.01'),\n",
       " Synset('source_book.n.01'),\n",
       " Synset('wordbook.n.01')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernyms()[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('entity.n.01'),\n",
       "  Synset('physical_entity.n.01'),\n",
       "  Synset('object.n.01'),\n",
       "  Synset('whole.n.02'),\n",
       "  Synset('artifact.n.01'),\n",
       "  Synset('creation.n.02'),\n",
       "  Synset('product.n.02'),\n",
       "  Synset('work.n.02'),\n",
       "  Synset('publication.n.01'),\n",
       "  Synset('book.n.01'),\n",
       "  Synset('reference_book.n.01'),\n",
       "  Synset('cookbook.n.01')]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernym_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### simplified part-of-speech tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tags can be used to look up specific Synsets for a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this number of synsets for great as a noun\n",
      " 1 \n",
      " this is number as adjective\n",
      " 6\n"
     ]
    }
   ],
   "source": [
    "print('this number of synsets for great as a noun\\n', \n",
    "      len(wordnet.synsets('great', pos='n')), '\\n this is number as adjective\\n',\n",
    "      len(wordnet.synsets('great', pos='a')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemma (in linguistics), is the canonical form or morphological form of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('cookbook')[0]\n",
    "lemmas = syn.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 2 forms or lemmas for cookbook and they are equivalent or synset\n",
      "cookbook\n",
      "cookery_book\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print('there are 2 forms or lemmas for cookbook and they are equivalent or synset')\n",
    "print(lemmas[0].name())\n",
    "print(lemmas[1].name())\n",
    "print(lemmas[1].synset()== lemmas[0].synset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All possible synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms = []\n",
    "for syn in wordnet.synsets('book'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "len(synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn2 = wordnet.synset('good.n.02')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moral excellence or admirableness'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gn2.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "evil = gn2.lemmas()[0].antonyms()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'evil'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evil.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quality of being morally wrong in principle or practice'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evil.synset().definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'having desirable or positive qualities especially those suitable for a thing specified'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ga1 = wordnet.synset('good.a.01')\n",
    "ga1.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad = ga1.lemmas()[0].antonyms()[0]\n",
    "bad.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'having undesirable or negative qualities'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad.synset().definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate Synset similarity\n",
    "The closer the two Synsets are in the tree, the more similar they are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**wup_similarity** method is short for Wu-Palmer Similarity  core metrics used to calculate similarity is the shortest path distance between the two Synsets and their common hypernym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "cb = wordnet.synset('cookbook.n.01')\n",
    "ib = wordnet.synset('instruction_book.n.01')\n",
    "cb.wup_similarity(ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = cb.hypernyms()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.shortest_path_distance(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ib.shortest_path_distance(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.shortest_path_distance(ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-31-14d69e202c92>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-14d69e202c92>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    comparing two dissimilar words\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "comparing two dissimilar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "becuase the share the synset _entity_ they are similar  0.38095238095238093\n"
     ]
    }
   ],
   "source": [
    "dog = wordnet.synsets('dog')[0]\n",
    "print('becuase the share the synset _entity_ they are similar ', dog.wup_similarity(cb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('whole.n.02')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dog.common_hypernyms(cb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cook = wordnet.synset('cook.v.01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bake = wordnet.synset('bake.v.02')\n",
    "cook.wup_similarity(bake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigramCollocationFinder\n",
    "most common bigrams in Monty Python and the Holy Grail. \n",
    "***Collocations are two or more words that tend to appear frequently together***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/webtext.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w.lower() for w in webtext.words('grail.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcf = BigramCollocationFinder.from_words(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'\", 's'), ('arthur', ':'), ('#', '1'), (\"'\", 't')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***adding a word filter to remove punctuation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('black', 'knight'),\n",
       " ('clop', 'clop'),\n",
       " ('head', 'knight'),\n",
       " ('mumble', 'mumble')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"BigramCollocationFinder constructs two frequency distributions: one for each word,\n",
    "and another for bigrams. A frequency distribution, or FreqDist in NLTK, is basically an\n",
    "enhanced Python dictionary where the keys are what's being counted, and the values are\n",
    "the counts. Any filtering functions that are applied reduce the size of these two FreqDists\n",
    "by eliminating any words that don't pass the filter. By using a filtering function to eliminate all\n",
    "words that are one or two characters, and all English stopwords, we can get a much cleaner\n",
    "result. After filtering, the collocation finder is ready to accept a generic scoring function for\n",
    "finding collocations.\"\"\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopset = set(stopwords.words('english'))\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "bcf.apply_word_filter(filter_stops)\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TrigramCollocationFinder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('long', 'term', 'relationship')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "words = [w.lower() for w in webtext.words('singles.txt')]\n",
    "tcf = TrigramCollocationFinder.from_words(words)\n",
    "tcf.apply_word_filter(filter_stops)\n",
    "tcf.apply_freq_filter(3)\n",
    "tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Scoring ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
