{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming is a technique to remove suffiixes from a word, ending up with the stem. \n",
    "most common stemming algorithms is the Porter stemming algorithm by Martin\n",
    "Porter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking stem=  cook\n",
      "cookery stem=  cookeri\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print('cooking stem= ', stemmer.stem('cooking'))\n",
    "print('cookery stem= ', stemmer.stem('cookery'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"stemmerI.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** LancasterStemmer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking stem=  cook\n",
      "cookery stem=  cookery\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "print('cooking stem= ', stemmer.stem('cooking'))\n",
    "print('cookery stem= ', stemmer.stem('cookery'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** RegexpStemmer clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex cooking stem=  cook\n",
      "regex cookery stem=  cookery\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "stemmer = RegexpStemmer('ing')\n",
    "print('regex cooking stem= ', stemmer.stem('cooking'))\n",
    "print('regex cookery stem= ', stemmer.stem('cookery'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** The SnowballStemmer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-34aa169f80fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spanish'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f8dec2381b3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m SnowballStemmer.languages('danish', 'dutch', 'english', 'finnish', 'french',\n\u001b[1;32m      3\u001b[0m                           \u001b[0;34m'german'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hungarian'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'italian'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norwegian'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'porter'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                           'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mspanish_stemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spanish'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mspanish_stemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hola'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "spanish_stemmer = SnowballStemmer('spanish')\n",
    "spanish_stemmer.stem('hola')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization is very similar to stemming, but is more akin to synonym replacement.\n",
    "always left with a valid word that means the same thing. However, the word you end up with can\n",
    "be completely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is lemmatizing \"cooking\" without POS; \n",
      " cooking\n",
      "this is lemmatizing \"cooking\" with POS=v; \n",
      " cook\n"
     ]
    }
   ],
   "source": [
    "'''This is because the default\n",
    "POS is a noun, and as a noun, cooking is its own lemma. On the other hand, cookbooks\n",
    "is a noun with its singular form, cookbook, as its lemma.\n",
    "'''\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print('''this is lemmatizing \"cooking\" without POS; \\n''',lemmatizer.lemmatize('cooking'))\n",
    "print('''this is lemmatizing \"cooking\" with POS=v; \\n''',lemmatizer.lemmatize('cooking',\n",
    "                                                                              pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining stemming with lemmatization  \n",
    " stemming saves one character, lemmatization saves two characters, and\n",
    "stemming the lemma saves a total of three characters out of five characters. That is nearly\n",
    "a 60% compression rate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  word replacement can be thought of as error correction or text normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recipe aims to fix this by replacing contractions with their expanded forms, for example, by ***replacing \"can't\" with \"cannot\" or \"would've\" with \"would have\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key things to know are matching patterns and the re.sub() function\n",
    "1.  ***define a number of replacement patterns*** This will be a list of tuple\n",
    "pairs, where the first element is the pattern to match with and the second element is\n",
    "the replacement.\n",
    "2.  ***create a RegexpReplacer class*** that will compile the patterns and provide\n",
    "a replace() method to substitute all the found patterns with their replacements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "import re\n",
    "replacement_patterns = [\n",
    "(r'won\\'t', 'will not'),\n",
    "(r'can\\'t', 'cannot'),\n",
    "(r'i\\'m', 'i am'),\n",
    "(r'ain\\'t', 'is not'),\n",
    "(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "(r'(\\w+)\\'d', '\\g<1> would')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alessandropiccolo/Google Drive/Python/1JupyterNotebook/NLTK'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing replacers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile replacers.py\n",
    "import re , csv , enchant #, yaml \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "##################################################\n",
    "## Replacing Words Matching Regular Expressions ##\n",
    "##################################################\n",
    "\n",
    "replacement_patterns = [\n",
    "\t(r'won\\'t', 'will not'),\n",
    "\t(r'can\\'t', 'cannot'),\n",
    "\t(r'i\\'m', 'i am'),\n",
    "\t(r'ain\\'t', 'is not'),\n",
    "\t(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "\t(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "\t(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "\t(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "\t(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "\t(r'(\\w+)\\'d', '\\g<1> would'),\n",
    "]\n",
    "\n",
    "class RegexpReplacer(object):\n",
    "\t\"\"\" Replaces regular expression in a text.\n",
    "\t>>> replacer = RegexpReplacer()\n",
    "\t>>> replacer.replace(\"can't is a contraction\")\n",
    "\t'cannot is a contraction'\n",
    "\t>>> replacer.replace(\"I should've done that thing I didn't do\")\n",
    "\t'I should have done that thing I did not do'\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, patterns=replacement_patterns):\n",
    "\t\tself.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "\t\n",
    "\tdef replace(self, text):\n",
    "\t\ts = text\n",
    "\t\t\n",
    "\t\tfor (pattern, repl) in self.patterns:\n",
    "\t\t\ts = re.sub(pattern, repl, s)\n",
    "\t\t\n",
    "\t\treturn s\n",
    "\n",
    "####################################\n",
    "## Replacing Repeating Characters ##\n",
    "####################################\n",
    "\n",
    "class RepeatReplacer(object):\n",
    "\t\"\"\" Removes repeating characters until a valid word is found.\n",
    "\t>>> replacer = RepeatReplacer()\n",
    "\t>>> replacer.replace('looooove')\n",
    "\t'love'\n",
    "\t>>> replacer.replace('oooooh')\n",
    "\t'ooh'\n",
    "\t>>> replacer.replace('goose')\n",
    "\t'goose'\n",
    "\t\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tself.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "\t\tself.repl = r'\\1\\2\\3'\n",
    "\n",
    "\tdef replace(self, word):\n",
    "\t\tif wordnet.synsets(word):\n",
    "\t\t\treturn word\n",
    "\t\t\n",
    "\t\trepl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "\t\t\n",
    "\t\tif repl_word != word:\n",
    "\t\t\treturn self.replace(repl_word)\n",
    "\t\telse:\n",
    "\t\t\treturn repl_word\n",
    "\n",
    "######################################\n",
    "## Spelling Correction with Enchant ##\n",
    "######################################\n",
    "\n",
    "class SpellingReplacer(object):\n",
    "\t\"\"\" Replaces misspelled words with a likely suggestion based on shortest\n",
    "\tedit distance.\n",
    "\t>>> replacer = SpellingReplacer()\n",
    "\t>>> replacer.replace('cookbok')\n",
    "\t'cookbook'\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, dict_name='en', max_dist=2):\n",
    "\t\tself.spell_dict = enchant.Dict(dict_name)\n",
    "\t\tself.max_dist = max_dist\n",
    "\t\n",
    "\tdef replace(self, word):\n",
    "\t\tif self.spell_dict.check(word):\n",
    "\t\t\treturn word\n",
    "\t\t\n",
    "\t\tsuggestions = self.spell_dict.suggest(word)\n",
    "\t\t\n",
    "\t\tif suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
    "\t\t\treturn suggestions[0]\n",
    "\t\telse:\n",
    "\t\t\treturn word\n",
    "\n",
    "class CustomSpellingReplacer(SpellingReplacer):\n",
    "\t\"\"\" SpellingReplacer that allows passing a custom enchant dictionary, such\n",
    "\ta DictWithPWL.\n",
    "\t>>> d = enchant.DictWithPWL('en_US', 'mywords.txt')\n",
    "\t>>> replacer = CustomSpellingReplacer(d)\n",
    "\t>>> replacer.replace('nltk')\n",
    "\t'nltk'\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, spell_dict, max_dist=2):\n",
    "\t\tself.spell_dict = spell_dict\n",
    "\t\tself.max_dist = max_dist\n",
    "\n",
    "########################\n",
    "## Replacing Synonyms ##\n",
    "########################\n",
    "\n",
    "class WordReplacer(object):\n",
    "\t\"\"\" WordReplacer that replaces a given word with a word from the word_map,\n",
    "\tor if the word isn't found, returns the word as is.\n",
    "\t>>> replacer = WordReplacer({'bday': 'birthday'})\n",
    "\t>>> replacer.replace('bday')\n",
    "\t'birthday'\n",
    "\t>>> replacer.replace('happy')\n",
    "\t'happy'\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, word_map):\n",
    "\t\tself.word_map = word_map\n",
    "\t\n",
    "\tdef replace(self, word):\n",
    "\t\treturn self.word_map.get(word, word)\n",
    "\n",
    "class CsvWordReplacer(WordReplacer):\n",
    "\t\"\"\" WordReplacer that reads word mappings from a csv file.\n",
    "\t>>> replacer = CsvWordReplacer('synonyms.csv')\n",
    "\t>>> replacer.replace('bday')\n",
    "\t'birthday'\n",
    "\t>>> replacer.replace('happy')\n",
    "\t'happy'\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, fname):\n",
    "\t\tword_map = {}\n",
    "\t\t\n",
    "\t\tfor line in csv.reader(open(fname)):\n",
    "\t\t\tword, syn = line\n",
    "\t\t\tword_map[word] = syn\n",
    "\t\t\n",
    "\t\tsuper(CsvWordReplacer, self).__init__(word_map)\n",
    "'''\n",
    "class YamlWordReplacer(WordReplacer):\n",
    "\t\"\"\" WordReplacer that reads word mappings from a yaml file.\n",
    "\t>>> replacer = YamlWordReplacer('synonyms.yaml')\n",
    "\t>>> replacer.replace('bday')\n",
    "\t'birthday'\n",
    "\t>>> replacer.replace('happy')\n",
    "\t'happy'\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, fname):\n",
    "\t\tword_map = yaml.load(open(fname))\n",
    "\t\tsuper(YamlWordReplacer, self).__init__(word_map)\n",
    "\n",
    "#######################################\n",
    "## Replacing Negations with Antonyms ##\n",
    "#######################################\n",
    "'''\n",
    "class AntonymReplacer(object):\n",
    "\tdef replace(self, word, pos=None):\n",
    "\t\t\"\"\" Returns the antonym of a word, but only if there is no ambiguity.\n",
    "\t\t>>> replacer = AntonymReplacer()\n",
    "\t\t>>> replacer.replace('good')\n",
    "\t\t>>> replacer.replace('uglify')\n",
    "\t\t'beautify'\n",
    "\t\t>>> replacer.replace('beautify')\n",
    "\t\t'uglify'\n",
    "\t\t\"\"\"\n",
    "\t\tantonyms = set()\n",
    "\t\t\n",
    "\t\tfor syn in wordnet.synsets(word, pos=pos):\n",
    "\t\t\tfor lemma in syn.lemmas():\n",
    "\t\t\t\tfor antonym in lemma.antonyms():\n",
    "\t\t\t\t\tantonyms.add(antonym.name())\n",
    "\t\t\n",
    "\t\tif len(antonyms) == 1:\n",
    "\t\t\treturn antonyms.pop()\n",
    "\t\telse:\n",
    "\t\t\treturn None\n",
    "\t\n",
    "\tdef replace_negations(self, sent):\n",
    "\t\t\"\"\" Try to replace negations with antonyms in the tokenized sentence.\n",
    "\t\t>>> replacer = AntonymReplacer()\n",
    "\t\t>>> replacer.replace_negations(['do', 'not', 'uglify', 'our', 'code'])\n",
    "\t\t['do', 'beautify', 'our', 'code']\n",
    "\t\t>>> replacer.replace_negations(['good', 'is', 'not', 'evil'])\n",
    "\t\t['good', 'is', 'not', 'evil']\n",
    "\t\t\"\"\"\n",
    "\t\ti, l = 0, len(sent)\n",
    "\t\twords = []\n",
    "\t\t\n",
    "\t\twhile i < l:\n",
    "\t\t\tword = sent[i]\n",
    "\t\t\t\n",
    "\t\t\tif word == 'not' and i+1 < l:\n",
    "\t\t\t\tant = self.replace(sent[i+1])\n",
    "\t\t\t\t\n",
    "\t\t\t\tif ant:\n",
    "\t\t\t\t\twords.append(ant)\n",
    "\t\t\t\t\ti += 2\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\twords.append(word)\n",
    "\t\t\ti += 1\n",
    "\t\t\n",
    "\t\treturn words\n",
    "\n",
    "class AntonymWordReplacer(WordReplacer, AntonymReplacer):\n",
    "\t\"\"\" AntonymReplacer that uses a custom mapping instead of WordNet.\n",
    "\tOrder of inheritance is very important, this class would not work if\n",
    "\tAntonymReplacer comes before WordReplacer.\n",
    "\t>>> replacer = AntonymWordReplacer({'evil': 'good'})\n",
    "\t>>> replacer.replace_negations(['good', 'is', 'not', 'evil'])\n",
    "\t['good', 'is', 'good']\n",
    "\t\"\"\"\n",
    "\tpass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\timport doctest\n",
    "\tdoctest.testmod()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/home/jovyan/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/jovyan/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting pyenchant\n",
      "\u001b[33m  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fe27db5d908>: Failed to establish a new connection: [Errno -2] Name or service not known',)': /simple/pyenchant/\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/54/04d88a59efa33fefb88133ceb638cdf754319030c28aadc5a379d82140ed/pyenchant-2.0.0.tar.gz (64kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 1.5MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pyenchant\n",
      "  Running setup.py install for pyenchant ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed pyenchant-2.0.0\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyenchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''simple usage example:'''\n",
    "import enchant\n",
    "import replacers\n",
    "\n",
    "from replacers import RegexpReplacer\n",
    "replacer = RegexpReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RegexpReplacer class created for \"can't is a contraction\"\n",
      " cannot is a contraction\n",
      "Using RegexpReplacer class created for \"I should've done that thing I didn't do\"\n",
      " I should have done that thing I did not do\n"
     ]
    }
   ],
   "source": [
    "print('''Using RegexpReplacer class created for \"can't is a contraction\"\\n''',\n",
    "      replacer.replace(\"can't is a contraction\"))\n",
    "print('''Using RegexpReplacer class created for \"I should've done that thing I didn't do\"\\n''',\n",
    "      replacer.replace(\"I should've done that thing I didn't do\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using the RegexpReplacer class as a preliminary step before tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is without regexReplacer \n",
      " ['ca', \"n't\", 'is', 'a', 'contraction']\n",
      "this is with regexReplacer \n",
      " ['can', 'not', 'is', 'a', 'contraction']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from replacers import RegexpReplacer\n",
    "replacer = RegexpReplacer()\n",
    "print('''this is without regexReplacer \\n''',\n",
    "      word_tokenize(\"can't is a contraction\"))\n",
    "print('''this is with regexReplacer \\n''',\n",
    "      word_tokenize(replacer.replace(\"can't is a contraction\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing repeating characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writefile replacers.py\n",
    "import re\n",
    "\n",
    "class RepeatReplacer(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return repl_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replacers import RepeatReplacer\n",
    "replacer = RepeatReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable         Type      Data/Info\n",
      "------------------------------------\n",
      "RepeatReplacer   type      <class '__main__.RepeatReplacer'>\n",
      "re               module    <module 're' from '/Users<...>nda/lib/python3.5/re.py'>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacer.replace('looooove')\n",
      " love\n",
      "replacer.replace('oooooh')\n",
      " ooh\n",
      "replacer.replace('goose')\n",
      " goose\n"
     ]
    }
   ],
   "source": [
    "print('''replacer.replace('looooove')\\n''', replacer.replace('looooove'))\n",
    "print('''replacer.replace('oooooh')\\n''',replacer.replace('oooooh'))\n",
    "print('''replacer.replace('goose')\\n''',replacer.replace('goose'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing synonyms Page 56\n",
    "This allows for compressing vocabulary without losgin meaning to save memory and inprove frequency analysis\n",
    "\n",
    "     store the synonyms in a CSV file or in a YAML file\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'synonyms.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a7a61eb71642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m''' defined mapping of a word to its synonym in the csv file.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreplacers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCsvWordReplacer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreplacer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCsvWordReplacer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'synonyms.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'replace bday with synonym in csvReplacer\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bday'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'when not in synonym in csvReplacer\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'happy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/replacers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0mword_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m                         \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                         \u001b[0mword_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msyn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'synonyms.csv'"
     ]
    }
   ],
   "source": [
    "''' defined mapping of a word to its synonym in the csv file.'''\n",
    "from replacers import CsvWordReplacer\n",
    "replacer = CsvWordReplacer('synonyms.csv')\n",
    "print('replace bday with synonym in csvReplacer\\n', replacer.replace('bday'))\n",
    "print('when not in synonym in csvReplacer\\n', replacer.replace('happy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing negations with antonyms\n",
    "created an AntonymReplacer class in replacers.py as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the original sent = \"let's\", 'not', 'uglify', 'our', 'code'\"\n",
      ", [\"let's\", 'beautify', 'our', 'code']\n"
     ]
    }
   ],
   "source": [
    "from replacers import AntonymReplacer\n",
    "replacer = AntonymReplacer()\n",
    "replacer.replace('good')\n",
    "replacer.replace('uglify')\n",
    "sent = [\"let's\", 'not', 'uglify', 'our', 'code']\n",
    "print('''the original sent = \"let's\", 'not', 'uglify', 'our', 'code'\"\\n,''',\n",
    "      replacer.replace_negations(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  new class called SpellingReplacer in replacers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookbook'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from replacers import SpellingReplacer\n",
    "#replacer = SpellingReplacer()\n",
    "replacerSP = SpellingReplacer()\n",
    "replacerSP.replace('cookbok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language', 'languages', 'languor', \"language's\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Here is an example showing all the suggestions for languege, a\n",
    "misspelling of language:  language, all the other words have an edit distance of\n",
    "three or greater. '''\n",
    "\n",
    "import enchant\n",
    "d = enchant.Dict('en')\n",
    "d.suggest('languege')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance  1\n",
      "distance 3\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "print('distance ', edit_distance('language', 'languege'))\n",
    "print('distance', edit_distance('language', 'languo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal word lists  \n",
    "This CustomSpellingReplacer class will not replace any words that you put into mywords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nltk'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import enchant\n",
    "from replacers import CustomSpellingReplacer\n",
    "d = enchant.DictWithPWL('en_US', 'mywords.txt')\n",
    "replacer = CustomSpellingReplacer(d)\n",
    "replacer.replace('nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                 Type                      Data/Info\n",
      "------------------------------------------------------------\n",
      "AntonymReplacer          type                      <class 'replacers.AntonymReplacer'>\n",
      "CsvWordReplacer          type                      <class 'replacers.CsvWordReplacer'>\n",
      "CustomSpellingReplacer   type                      <class 'replacers.CustomSpellingReplacer'>\n",
      "LancasterStemmer         ABCMeta                   <class 'nltk.stem.lancaster.LancasterStemmer'>\n",
      "PorterStemmer            ABCMeta                   <class 'nltk.stem.porter.PorterStemmer'>\n",
      "RegexpReplacer           type                      <class 'replacers.RegexpReplacer'>\n",
      "RegexpStemmer            ABCMeta                   <class 'nltk.stem.regexp.RegexpStemmer'>\n",
      "RepeatReplacer           type                      <class 'replacers.RepeatReplacer'>\n",
      "SnowballStemmer          ABCMeta                   <class 'nltk.stem.snowball.SnowballStemmer'>\n",
      "SpellingReplacer         type                      <class 'replacers.SpellingReplacer'>\n",
      "WordNetLemmatizer        type                      <class 'nltk.stem.wordnet.WordNetLemmatizer'>\n",
      "d                        DictWithPWL               <enchant.DictWithPWL object at 0x7fa188d80c88>\n",
      "edit_distance            function                  <function edit_distance at 0x7fa195267ae8>\n",
      "enchant                  module                    <module 'enchant' from '/<...>ges/enchant/__init__.py'>\n",
      "lemmatizer               WordNetLemmatizer         <WordNetLemmatizer>\n",
      "re                       module                    <module 're' from '/opt/c<...>nda/lib/python3.6/re.py'>\n",
      "replacement_patterns     list                      n=10\n",
      "replacer                 CustomSpellingReplacer    <replacers.CustomSpelling<...>object at 0x7fa188de0fd0>\n",
      "replacerSP               SpellingReplacer          <replacers.SpellingReplac<...>object at 0x7fa188de0630>\n",
      "replacers                module                    <module 'replacers' from <...>/notebooks/replacers.py'>\n",
      "sent                     list                      n=5\n",
      "stemmer                  RegexpStemmer             <RegexpStemmer: 'ing'>\n",
      "word_tokenize            function                  <function word_tokenize at 0x7fa18f7ae048>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
