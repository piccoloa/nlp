{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a custom corpus (bunch of text files in directory)\n",
    "\n",
    "to train your own model, such as a part-of-speech tagger or text classifier, you will need to\n",
    "create a custom corpus to train on.   \n",
    "    ***create a custom nltk_data directory in our home directory***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created in /Users/alessandropiccolo/nltk_data\n",
    "import os, os.path\n",
    "#defines name of new folder in users directory\n",
    "path = os.path.expanduser('/notebooks/nltk_data')\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test that directory was created correctly\n",
    "import os, os.path\n",
    "path = os.path.expanduser('/notebooks/nltk_data')\n",
    "os.path.exists(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"test that custom directory 'Users/alessandropiccolo/nltk_data' is in \n",
    "nltk.data.path required to uses as customer corpus\n",
    "\"\"\"\n",
    "import nltk.data\n",
    "path in nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Once you have your nltk_data directory,\n",
    "the convention is that corpora resides in a corpora subdirectory. Create\n",
    "this corpora directory within the nltk_data directory, so that the path is\n",
    "~/nltk_data/corpora. '''#pg 50\n",
    "import os, os.path\n",
    "path = os.path.expanduser('~/work/nltk_data/corpora')\n",
    "#path2 = os.path.expanduser('~/nltk_data/corpora/cookbook')\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a simple wordlist file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alessandropiccolo/Google Drive/Python/1JupyterNotebook/NLTK'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"'nltk'\\n'corpus'\\n'corpora'\\n'wordnet'\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "nltk.data.load('/home/jovyan/work/nltk_data/corpora/cookbook/mywords.txt', format='raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a wordlist corpus  \n",
    "provides access to a file containing a list of words, one word per line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'nltk'\", \"'corpus'\", \"'corpora'\", \"'wordnet'\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"instantiate a WordListCorpusReader class that will produce a list of words\"\"\"\n",
    "from nltk.corpus.reader import WordListCorpusReader\n",
    "reader = WordListCorpusReader('/home/jovyan/work/nltk_data/corpora/cookbook', ['mywords.txt'])\n",
    "reader.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mywords.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating a POS tagged word corpus\n",
    "<img src=\"taggedCorpusReader.png\" />\n",
    "file called brown.pos, you could then create a TaggedCorpusReader class using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'expense', 'and', 'time', 'involved', 'are', ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus.reader import TaggedCorpusReader\n",
    "reader = TaggedCorpusReader('/Users/alessandropiccolo/nltk_data/corpora/cookbook', r'.*\\.pos')\n",
    "reader.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT-TL'), ('expense', 'NN'), ('and', 'CC'), ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"list of tagged tokens. A tagged token is simply\n",
    "a tuple of (word, tag)\"\"\"\n",
    "reader.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'expense', 'and', 'time', 'involved', 'are', 'astronomical', '.']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"list of every sentence and also every tagged\n",
    "sentence where the sentence is itself a list of words or tagged tokens\"\"\"\n",
    "reader.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'AT-TL'), ('expense', 'NN'), ('and', 'CC'), ('time', 'NN'), ('involved', 'VBN'), ('are', 'BER'), ('astronomical', 'JJ'), ('.', '.')]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"list of paragraphs, where each paragraph is a list of sentences and each sentence is a list of\n",
    "words or tagged tokens\"\"\"\n",
    "reader.tagged_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customize\n",
    "them by passing in your own tokenizers, sentence tokenizer, paragraph at the time of initialization.  Page 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'expense', 'and', 'time', 'involved', 'are', ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import SpaceTokenizer\n",
    "reader = TaggedCorpusReader('/Users/alessandropiccolo/nltk_data/corpora/cookbook', r'.*\\.pos',\n",
    "                            word_tokenizer=SpaceTokenizer())\n",
    "reader.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a corpus with sentences that contain chunks -- creating chunked phrases  \n",
    "chunk is a short phrase within a sentence  chunks are: subtrees within a sentence tree, and they will be covered in much more detail  in Chapter 5, Extracting Chunks  \n",
    "Words that are not within brackets are part of the sentence tree, but are not part of any noun phrase subtree.\n",
    ">[Earlier/JJR staff-reduction/NN moves/NNS] have/VBP trimmed/VBN about/\n",
    "IN [300/CD jobs/NNS] ,/, [the/DT spokesman/NN] said/VBD ./."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked words\n",
      " [Tree('NP', [('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]), ('have', 'VBP'), ...]\n",
      "chuncked sentences\n",
      " [Tree('S', [Tree('NP', [('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]), ('have', 'VBP'), ('trimmed', 'VBN'), ('about', 'IN'), Tree('NP', [('300', 'CD'), ('jobs', 'NNS')]), (',', ','), Tree('NP', [('the', 'DT'), ('spokesman', 'NN')]), ('said', 'VBD'), ('.', '.')])]\n",
      "chunked paragraphs\n",
      " [[Tree('S', [Tree('NP', [('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]), ('have', 'VBP'), ('trimmed', 'VBN'), ('about', 'IN'), Tree('NP', [('300', 'CD'), ('jobs', 'NNS')]), (',', ','), Tree('NP', [('the', 'DT'), ('spokesman', 'NN')]), ('said', 'VBD'), ('.', '.')])]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader import ChunkedCorpusReader\n",
    "reader = ChunkedCorpusReader('.', r'.*\\.chunk')\n",
    "print('chunked words\\n', reader.chunked_words())\n",
    "print('chuncked sentences\\n', reader.chunked_sents())\n",
    "print('chunked paragraphs\\n',reader.chunked_paras())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree leaves  \n",
    " leaves of a tree are the tagged tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaves [('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]\n",
      "leaves of sents\n",
      " [('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS'), ('have', 'VBP'), ('trimmed', 'VBN'), ('about', 'IN'), ('300', 'CD'), ('jobs', 'NNS'), (',', ','), ('the', 'DT'), ('spokesman', 'NN'), ('said', 'VBD'), ('.', '.')]\n",
      "leaves of para \n",
      " [('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS'), ('have', 'VBP'), ('trimmed', 'VBN'), ('about', 'IN'), ('300', 'CD'), ('jobs', 'NNS'), (',', ','), ('the', 'DT'), ('spokesman', 'NN'), ('said', 'VBD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print('leaves', reader.chunked_words()[0].leaves())\n",
    "print('leaves of sents\\n', reader.chunked_sents()[0].leaves())\n",
    "print('leaves of para \\n', reader.chunked_paras()[0][0].leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  allowing multiple different chunk phrase types, not just noun phrases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conllreader chunked words\n",
      " [Tree('NP', [('Mr.', 'NNP'), ('Meador', 'NNP')]), Tree('VP', [('had', 'VBD'), ('been', 'VBN')]), ...]\n",
      "conllreader chunked sents\n",
      " [Tree('S', [Tree('NP', [('Mr.', 'NNP'), ('Meador', 'NNP')]), Tree('VP', [('had', 'VBD'), ('been', 'VBN')]), Tree('NP', [('executive', 'JJ'), ('vice', 'NN'), ('president', 'NN')]), Tree('PP', [('of', 'IN')]), Tree('NP', [('Balcor', 'NNP')]), ('.', '.')])]\n",
      "conllreader chunked into list of tupples word\n",
      " [('Mr.', 'NNP', 'B-NP'), ('Meador', 'NNP', 'I-NP'), ...]\n",
      "conllreader chunked into list of tupples sent\n",
      " [[('Mr.', 'NNP', 'B-NP'), ('Meador', 'NNP', 'I-NP'), ('had', 'VBD', 'B-VP'), ('been', 'VBN', 'I-VP'), ('executive', 'JJ', 'B-NP'), ('vice', 'NN', 'I-NP'), ('president', 'NN', 'I-NP'), ('of', 'IN', 'B-PP'), ('Balcor', 'NNP', 'B-NP'), ('.', '.', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader import ConllChunkCorpusReader\n",
    "conllreader = ConllChunkCorpusReader('/Users/alessandropiccolo/nltk_data/corpora/cookbook', r'.*\\.iob',\n",
    "                                     ('NP', 'VP', 'PP'))\n",
    "print('conllreader chunked words\\n', conllreader.chunked_words())\n",
    "print('conllreader chunked sents\\n', conllreader.chunked_sents())\n",
    "print('conllreader chunked into list of tupples word\\n', conllreader.iob_words())\n",
    "print('conllreader chunked into list of tupples sent\\n', conllreader.iob_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
