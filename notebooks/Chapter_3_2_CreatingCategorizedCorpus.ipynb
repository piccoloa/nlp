{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a categorized text corpus  \n",
    "brown corpus, for example  \n",
    "Categorized corpora  \n",
    "The movie_reviews corpus reader is an instance of\n",
    "CategorizedPlaintextCorpusReader, as is the reuters corpus reader. But where the\n",
    "movie_reviews corpus only has two categories (neg and pos), reuters has 90 categories.\n",
    "These corpora are often used for training and evaluating classifiers, which will be covered in\n",
    "Chapter 7, Text Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"catgorizedTextProcessing.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" CategorizedPlaintextCorpusReader class, which inherits from\n",
    "both PlaintextCorpusReader and CategorizedCorpusReader. These two\n",
    "superclasses require three arguments: the root directory, the fileids arguments,\n",
    "and a category specification:\"\"\"\n",
    "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
    "\"\"\"CategorizedPlaintextCorpusReader('file location', r'filename_wildcard.text', cat_pattern=r'text_space.txt)\"\"\"\n",
    "reader = CategorizedPlaintextCorpusReader('/Users/alessandropiccolo/nltk_data/corpora/cookbook',\n",
    "                                          r'movie_.*\\.txt', cat_pattern=r'movie_(\\w+)\\.txt')\n",
    "\n",
    "\n",
    "reader.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"alternative to cat_pattern\"\"\"\n",
    "reader = CategorizedPlaintextCorpusReader('/Users/alessandropiccolo/nltk_data/corpora/cookbook',\n",
    "                                          r'movie_.*\\.txt',\n",
    "                                          cat_map={'movie_pos.txt': ['pos'], 'movie_neg.txt': ['neg']})\n",
    "reader.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                           Type                                Data/Info\n",
      "--------------------------------------------------------------------------------\n",
      "CategorizedPlaintextCorpusReader   type                                <class 'nltk.corpus.reade<...>edPlaintextCorpusReader'>\n",
      "brown                              CategorizedTaggedCorpusReader       <CategorizedTaggedCorpusR<...>nltk_data/corpora/brown'>\n",
      "reader                             CategorizedPlaintextCorpusReader    <CategorizedPlaintextCorp<...>k_data/corpora/cookbook'>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie_neg.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"cat_pattern keyword is passed to CategorizedCorpusReader, which overrides the\n",
    "common corpus reader functions such as fileids(), words(), sents(), and paras() to\n",
    "accept a categories keyword argument. This way, you could get all the pos sentences by\n",
    "calling reader.sents(categories=['pos']). The CategorizedCorpusReader class\n",
    "also provides the categories() function, which returns a list of all the known categories in\n",
    "the corp\"\"\"\n",
    "reader.fileids(categories=['neg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie_pos.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.fileids(categories=['pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a categorized chunk corpus reader  page 69\n",
    "\n",
    "create a class called CategorizedChunkedCorpusReader that inherits from both\n",
    "CategorizedCorpusReader and ChunkedCorpusReader. It is heavily based on the\n",
    "CategorizedTaggedCorpusReader class, and also provides three additional methods\n",
    "for getting categorized chunks. The following code is found in catchunked.py:\n",
    "\n",
    "> CategorizedChunkedCorpusReader class overrides all the ChunkedCorpusReader\n",
    "methods to take a categories argument for locating fileids. These fileids are\n",
    "found with the internal _resolve() function  This _resolve() function makes use\n",
    "of CategorizedCorpusReader.fileids() to return fileids for a given list of\n",
    "categories. If no categories are given, _resolve() just returns the given fileids,\n",
    "which could be None, in which case all the files are read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alessandropiccolo/Google Drive/Python/1JupyterNotebook/NLTK'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing catchunked.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile catchunked.py\n",
    "from nltk.corpus.reader import CategorizedCorpusReader, ChunkedCorpusReader\n",
    "from nltk.corpus.reader import ConllCorpusReader, ConllChunkCorpusReader\n",
    "\n",
    "class CategorizedChunkedCorpusReader(CategorizedCorpusReader, ChunkedCorpusReader):\n",
    "\t\"\"\"\n",
    "\tA reader for chunked corpora whose documents are divided into categories\n",
    "\tbased on their file identifiers.\n",
    "\t\"\"\"\n",
    "\t# code adapted from CategorizedTaggedCorpusReader\n",
    "\tdef __init__(self, *args, **kwargs):\n",
    "\t\tCategorizedCorpusReader.__init__(self, kwargs)\n",
    "\t\tChunkedCorpusReader.__init__(self, *args, **kwargs)\n",
    "\t\n",
    "\tdef _resolve(self, fileids, categories):\n",
    "\t\tif fileids is not None and categories is not None:\n",
    "\t\t\traise ValueError('Specify fileids or categories, not both')\n",
    "\t\tif categories is not None:\n",
    "\t\t\treturn self.fileids(categories)\n",
    "\t\telse:\n",
    "\t\t\treturn fileids\n",
    "\t\n",
    "\tdef raw(self, fileids=None, categories=None):\n",
    "\t\treturn ChunkedCorpusReader.raw(self, self._resolve(fileids, categories))\n",
    "\t\n",
    "\tdef words(self, fileids=None, categories=None):\n",
    "\t\treturn ChunkedCorpusReader.words(self, self._resolve(fileids, categories))\n",
    "\t\n",
    "\tdef sents(self, fileids=None, categories=None):\n",
    "\t\treturn ChunkedCorpusReader.sents(self, self._resolve(fileids, categories))\n",
    "\t\n",
    "\tdef paras(self, fileids=None, categories=None):\n",
    "\t\treturn ChunkedCorpusReader.paras(self, self._resolve(fileids, categories))\n",
    "\t\n",
    "\tdef tagged_words(self, fileids=None, categories=None):\n",
    "\t\treturn ChunkedCorpusReader.tagged_words(self, self._resolve(fileids, categories))\n",
    "\t\n",
    "\tdef tagged_sents(self, fileids=None, categories=None):\n",
    "\t\treturn ChunkedCorpusReader.tagged_sents(self, self._resolve(fileids, categories))\n",
    "\t\t\n",
    "\tdef tagged_paras(self, fileids=None, categories=None):\n",
    "\t\treturn ChunkedCorpusReader.tagged_paras(self, self._resolve(fileids, categories))\n",
    "\t\n",
    "\tdef chunked_words(self, fileids=None, categories=None):\n",
    "\t\treturn ChunkedCorpusReader.chunked_words(\n",
    "\t\t\tself, self._resolve(fileids, categories))\n",
    "\t\n",
    "\tdef chunked_sents(self, fileids=None, categories=None):\n",
    "\t\treturn ChunkedCorpusReader.chunked_sents(\n",
    "\t\t\tself, self._resolve(fileids, categories))\n",
    "\t\n",
    "\tdef chunked_paras(self, fileids=None, categories=None):\n",
    "\t\treturn ChunkedCorpusReader.chunked_paras(\n",
    "\t\t\tself, self._resolve(fileids, categories))\n",
    "class CategorizedConllChunkCorpusReader(CategorizedCorpusReader, ConllChunkCorpusReader):\n",
    "\t\"\"\"\n",
    "\tA reader for conll chunked corpora whose documents are divided into\n",
    "\tcategories based on their file identifiers.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, *args, **kwargs):\n",
    "\t\t# NOTE: in addition to cat_pattern, ConllChunkCorpusReader also requires\n",
    "\t\t# chunk_types as third argument, which defaults to ('NP','VP','PP')\n",
    "\t\tCategorizedCorpusReader.__init__(self, kwargs)\n",
    "\t\tConllChunkCorpusReader.__init__(self, *args, **kwargs)\n",
    "\t\n",
    "\tdef _resolve(self, fileids, categories):\n",
    "\t\tif fileids is not None and categories is not None:\n",
    "\t\t\traise ValueError('Specify fileids or categories, not both')\n",
    "\t\tif categories is not None:\n",
    "\t\t\treturn self.fileids(categories)\n",
    "\t\telse:\n",
    "\t\t\treturn fileids\n",
    "\t\n",
    "\tdef raw(self, fileids=None, categories=None):\n",
    "\t\treturn ConllCorpusReader.raw(self, self._resolve(fileids, categories))\n",
    "\t\n",
    "\tdef words(self, fileids=None, categories=None):\n",
    "\t\treturn ConllCorpusReader.words(self, self._resolve(fileids, categories))\n",
    "\t\n",
    "\tdef sents(self, fileids=None, categories=None):\n",
    "\t\treturn ConllCorpusReader.sents(self, self._resolve(fileids, categories))\n",
    "\t\n",
    "\tdef tagged_words(self, fileids=None, categories=None):\n",
    "\t\treturn ConllCorpusReader.tagged_words(self, self._resolve(fileids, categories))\n",
    "\t\n",
    "\tdef tagged_sents(self, fileids=None, categories=None):\n",
    "\t\treturn ConllCorpusReader.tagged_sents(self, self._resolve(fileids, categories))\n",
    "\t\n",
    "\tdef chunked_words(self, fileids=None, categories=None, chunk_types=None):\n",
    "\t\treturn ConllCorpusReader.chunked_words(\n",
    "\t\t\tself, self._resolve(fileids, categories), chunk_types)\n",
    "\t\n",
    "\tdef chunked_sents(self, fileids=None, categories=None, chunk_types=None):\n",
    "\t\treturn ConllCorpusReader.chunked_sents(\n",
    "\t\t\tself, self._resolve(fileids, categories), chunk_types)\n",
    "\t\n",
    "\tdef parsed_sents(self, fileids=None, categories=None, pos_in_tree=None):\n",
    "\t\treturn ConllCorpusReader.parsed_sents(\n",
    "\t\t\tself, self._resolve(fileids, categories), pos_in_tree)\n",
    "\t\n",
    "\tdef srl_spans(self, fileids=None, categories=None):\n",
    "\t\treturn ConllCorpusReader.srl_spans(self, self._resolve(fileids, categories))\n",
    "\t\n",
    "\tdef srl_instances(self, fileids=None, categories=None, pos_in_tree=None, flatten=True):\n",
    "\t\treturn ConllCorpusReader.srl_instances(\n",
    "\t\t\tself, self._resolve(fileids, categories), pos_in_tree, flatten)\n",
    "\t\n",
    "\tdef iob_words(self, fileids=None, categories=None):\n",
    "\t\treturn ConllCorpusReader.iob_words(self, self._resolve(fileids, categories))\n",
    "\t\n",
    "\tdef iob_sents(self, fileids=None, categories=None):\n",
    "\t\treturn ConllCorpusReader.iob_sents(self, self._resolve(fileids, categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making categories out of the fileids arguments..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "from catchunked import CategorizedChunkedCorpusReader\n",
    "path = nltk.data.find('corpora/treebank/tagged')\n",
    "\"\"\"uses name of files to categorize while doing the chuncking\"\"\"\n",
    "reader = CategorizedChunkedCorpusReader(path, r'wsj_.*\\.pos', cat_pattern=r'wsj_(.*)\\.pos')\n",
    "len(reader.categories()) == len(reader.fileids())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reader.chunked_sents(categories=['0001']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunk corpus using IOB tags  page 72 .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "from catchunked import CategorizedConllChunkCorpusReader\n",
    "path = nltk.data.find('corpora/conll2000')\n",
    "reader = CategorizedConllChunkCorpusReader(path, r'.*\\.txt', ('NP','VP','PP'), cat_pattern=r'(.*)\\.txt')\n",
    "reader.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test.txt', 'train.txt']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2012"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reader.chunked_sents(categories=['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a corpus reader can be an expensive operation due to the number of files, file sizes, and various initialization tasks\n",
    "***eliminating\n",
    "the overhead of loading the corpus reader immediately.***  \n",
    "\n",
    "***LazyCorpusLoader*** class that can transform itself into your actual corpus reader as soon as you need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"LazyCorpusLoader is a proxy object which is used to stand in for a\n",
    "    corpus object before the corpus is loaded.  This allows NLTK to\n",
    "    create an object for each corpus, but defer the costs associated\n",
    "    with loading those corpora until the first time that they're\n",
    "    actually accessed.\"\"\"\n",
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.corpus.reader import WordListCorpusReader\n",
    "#arguments 'directory of corpora', 'name of reader'[filename]\n",
    "reader = LazyCorpusLoader('cookbook', WordListCorpusReader,['wordlist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(reader, LazyCorpusLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wordlist']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"So in the previous example code, before we call reader.fileids(), reader is an\n",
    "instance of LazyCorpusLoader, but after the call, reader becomes an instance of\n",
    "WordListCorp\"\"\"\n",
    "isinstance(reader, LazyCorpusLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(reader, WordListCorpusReader)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
